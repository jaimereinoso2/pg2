{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kstgk2Ow9mme"
      },
      "source": [
        "\n",
        "\n",
        "### Generación de Modelos Predictivos con LOGISTIC REGRESSION\n",
        "\n",
        "Este tipo de modelos requiere una revisión previa de colienalidad.  Tambien permiten obtener información del peso de las variables en el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "# Cargar tu DataFrame, reemplaza 'tu_dataframe.csv' con el nombre de tu archivo CSV\n",
        "dfbase = pd.read_pickle('checkpoints/DATOS_LIMPIOS.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = dfbase.copy()\n",
        "# Dividir el DataFrame en variables independientes (X) y la variable dependiente (y)\n",
        "X = df.drop('LABEL', axis=1)\n",
        "y = df['LABEL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculamos correlación de variables independientes\n",
        "corr_matrix = X.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializa una lista para almacenar las parejas de variables y sus valores de correlación\n",
        "correlation_pairs = []\n",
        "\n",
        "# Obtén las dimensiones de la matriz de correlación\n",
        "num_rows, num_cols = corr_matrix.shape\n",
        "\n",
        "# Itera a través de las columnas de la matriz de correlación\n",
        "for i in range(num_cols):\n",
        "    for j in range(i + 1, num_cols):  # Evita duplicados y la diagonal principal\n",
        "        variable1 = corr_matrix.columns[i]\n",
        "        variable2 = corr_matrix.columns[j]\n",
        "        correlation_value = corr_matrix.iloc[i, j]\n",
        "        correlation_pairs.append((variable1, variable2, correlation_value))\n",
        "\n",
        "# Ordena la lista de parejas de variables por valor de correlación de mayor a menor\n",
        "correlation_pairs.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "dfcorr = pd.DataFrame(correlation_pairs,columns=['var1','var2','corr'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "temp = dfcorr.sort_values(by='corr', ascending=False)\n",
        "\n",
        "# Filtra las filas donde 'var1' es diferente de 'var2'\n",
        "dfcorr = temp[temp['var1'] != temp['var2']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "      Intervalo    Mínimo    Máximo  Cantidad\n",
            "0   (-1.0, 0.0]       NaN       NaN         0\n",
            "1    (0.0, 0.1] -0.099999  0.099998   1830411\n",
            "2    (0.1, 0.2] -0.199931  0.199994     47540\n",
            "3    (0.2, 0.3] -0.299742  0.299997     10871\n",
            "4    (0.3, 0.4] -0.398727  0.399766      3977\n",
            "5    (0.4, 0.5] -0.499354  0.499942      2562\n",
            "6    (0.5, 0.6] -0.598363  0.599702      2135\n",
            "7    (0.6, 0.7] -0.696373  0.699747       996\n",
            "8    (0.7, 0.8] -0.792244  0.799353      1437\n",
            "9    (0.8, 0.9] -0.857050  0.897663      1466\n",
            "10   (0.9, 1.0] -1.000000  1.000000      4141\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/tx/1tx9w9fj6v78n5f12zkwg_vc0000gn/T/ipykernel_47254/4287402673.py:7: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
            "  tabla_resumen = dfcorr.groupby('intervalo')['corr'].agg([min, max, 'count']).reset_index()\n",
            "/var/folders/tx/1tx9w9fj6v78n5f12zkwg_vc0000gn/T/ipykernel_47254/4287402673.py:7: FutureWarning: The provided callable <built-in function min> is currently using SeriesGroupBy.min. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"min\" instead.\n",
            "  tabla_resumen = dfcorr.groupby('intervalo')['corr'].agg([min, max, 'count']).reset_index()\n",
            "/var/folders/tx/1tx9w9fj6v78n5f12zkwg_vc0000gn/T/ipykernel_47254/4287402673.py:7: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n",
            "  tabla_resumen = dfcorr.groupby('intervalo')['corr'].agg([min, max, 'count']).reset_index()\n"
          ]
        }
      ],
      "source": [
        "intervalos = [-1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "\n",
        "# Utiliza pd.cut para dividir 'corr' en intervalos\n",
        "dfcorr['intervalo'] = pd.cut(dfcorr['corr'].abs(), bins=intervalos)\n",
        "\n",
        "# Agrupa y cuenta las filas en cada intervalo\n",
        "tabla_resumen = dfcorr.groupby('intervalo')['corr'].agg([min, max, 'count']).reset_index()\n",
        "\n",
        "# Renombra las columnas\n",
        "tabla_resumen.columns = ['Intervalo', 'Mínimo', 'Máximo', 'Cantidad']\n",
        "\n",
        "# Muestra la tabla resumen\n",
        "print(tabla_resumen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "348\n"
          ]
        }
      ],
      "source": [
        "# Filtra las filas donde 'corr' está entre 0 y 0.1\n",
        "resultado = dfcorr[(dfcorr['corr'] >= 0) & (dfcorr['corr'] <= 0.0001)]\n",
        "\n",
        "# Crea un conjunto de variables únicas en 'var1' y 'var2' en las filas filtradas\n",
        "conjunto_de_variables = set(resultado['var1']).union(set(resultado['var2']))\n",
        "\n",
        "# Muestra el conjunto de variables resultante\n",
        "print(len(conjunto_de_variables))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(7145, 1953)\n",
            "(7145, 1022)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Establece un umbral para la correlación\n",
        "umbral_correlacion = 0.5\n",
        "\n",
        "# Encuentra las columnas que tienen correlaciones por encima del umbral\n",
        "columnas_eliminar = set()\n",
        "for i in range(len(corr_matrix.columns)):\n",
        "    for j in range(i):\n",
        "        if abs(corr_matrix.iloc[i, j]) > umbral_correlacion:\n",
        "            colname = corr_matrix.columns[i]\n",
        "            columnas_eliminar.add(colname)\n",
        "\n",
        "# Elimina las columnas que pasan el umbral\n",
        "Xnew = X.drop(columns=columnas_eliminar)\n",
        "print(X.shape)\n",
        "print(Xnew.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Separar los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xnew, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Aplicar StandardScaler para estandarizar las características (opcional pero recomendado)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jaimereinoso/DESARROLLO/MCD/proyectodegrado2/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jaimereinoso/DESARROLLO/MCD/proyectodegrado2/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jaimereinoso/DESARROLLO/MCD/proyectodegrado2/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/Users/jaimereinoso/DESARROLLO/MCD/proyectodegrado2/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puntajes de validación cruzada: [0.74107768 0.70538838 0.7179846  0.75437369 0.71588523]\n",
            "Precisión media: 0.73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/jaimereinoso/DESARROLLO/MCD/proyectodegrado2/venv/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Entrenar el modelo de regresión logística\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "cv_scores = cross_val_score(model, X, y, cv=5)  # Utiliza 5 divisiones para validación cruzada\n",
        "\n",
        "# Imprimir los puntajes de validación cruzada\n",
        "print(\"Puntajes de validación cruzada:\", cv_scores)\n",
        "print(\"Precisión media: {:.2f}\".format(cv_scores.mean()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
